Generates a tight search query with your local Phi model.

Searches DuckDuckGo and saves the top 10 links.

Crawls those links with crawl4ai (headless, async).

Extracts page content (markdown/html/extracted JSON).

Asks the user which output they want (summary / pie chart / table / distinguish / mix) using Phi.

Produces summaries, topic tags, a CSV table, and a topic-distribution pie chart — all files are overwritten every run (no accidental appends).

Quick start (minimum steps)

Clone this repo.

Create and activate a venv (or conda env — recommended).

Install dependencies.

Put your local model + runner in place and update RUNNER / MODEL constants in the script if needed.

Run the script.

Example (pip + venv on Windows PowerShell):

py -3.12 -m venv .venv
.venv\Scripts\Activate.ps1

python -m pip install --upgrade pip setuptools wheel
python -m pip install -r requirements.txt

# Playwright required by crawl4ai for browser rendering:
python -m playwright install chromium

# run
python all_in_one_pipeline.py


If you prefer conda (easier binary packages, fewer build issues):

conda create -n topicgen python=3.12 -y
conda activate topicgen
conda install -c conda-forge python=3.12 pandas matplotlib requests beautifulsoup4 -y
pip install crawl4ai
python -m playwright install chromium
python all_in_one_pipeline.py

Requirements / dependencies

Put the following into requirements.txt (or install with pip):

crawl4ai
requests
beautifulsoup4
pandas
matplotlib
playwright


Notes:

crawl4ai requires Playwright and may spawn a headless browser (Chromium). Run python -m playwright install chromium after installing Playwright.

pandas and matplotlib require numpy. On Windows, building numpy from source is painful — use conda or install compatible wheels (see Troubleshooting).

This script was developed against Python 3.12 / 3.11; using 3.12 is recommended for wheel availability.

Files produced by the script (every run overwrites)

query.txt — the single optimized query generated by Phi. (write 'w' mode)

links.txt — top 10 DuckDuckGo links for that query. (write mode)

data/ directory:

data/data.json — aggregated crawl results (list of dicts). (write mode)

data/<safe_url>.md — per-page markdown when available (overwritten each run).

data/summaries.json — summaries + topic tags (when requested). (write mode)

data/table.csv — table url,title,summary,topic (when requested). (write mode)

data/topic_pie.png — pie chart of topic distribution (when requested). (write mode)

Everything is stored with overwrite behavior so you don't accumulate junk across runs.

Configuration (edit at top of script)
MODEL = "Phi-3-mini-4k-instruct-q4.gguf"   # change if different
RUNNER = "./windows/llama-run.exe"        # path to your local runner (llama-run/llama-runner)
CONTEXT = "2000"
THREADS = "2"
CONCURRENT_TASKS = 4
DELAY_BETWEEN_CRAWLS = 1.0


RUNNER must be callable from the script and accept positional prompt text the way you currently use it. If your runner uses different flags, change run_model() accordingly.

MODEL must point to the model file your runner expects.

How the flow works — full details

Warm up Phi — script sends a short system prompt to make the model ready.

User prompt — you type a short description of what you want to search for.

Phi generates the optimized search query (one-line, concise). The query gets written to query.txt (overwrite).

DuckDuckGo search — script posts to https://html.duckduckgo.com/html/?q=... and scrapes <a class="result__a"> items. Top 10 links saved to links.txt (overwrite).

Crawl — script runs crawl4ai asynchronously with CONCURRENT_TASKS workers. For each page it saves:

Per-page markdown file (if crawl4ai returns .markdown).

Aggregated data/data.json file with fields like url, title, markdown, html, extracted_content.

Phi asks the user which output format — it builds a one-line UX question (“1) summary 2) pie chart 3) table 4) distinguish 5) mix”) and prints it. You input your choice.

Outputs generated — depending on choice:

summary → data/summaries.json (list with url,title,summary,topic).

table → data/table.csv.

pie → data/topic_pie.png.

distinguish → saves summaries.json (useful for filtering).

mix → all of the above.

All model outputs are generated by passing article text to local Phi (with content truncated to avoid context overflow). Each article receives a summary and a single short topic tag.

Ethics & scraping rules (important)

Respect robots.txt and site terms of use. This script does not check robots.txt automatically. Use it responsibly.

Be polite: DELAY_BETWEEN_CRAWLS is set to 1s by default; lower concurrency or add longer sleeps for fragile targets.

Do not use this to scrape private, paywalled, or copyrighted content in violation of terms.

Troubleshooting — common issues & fixes

1) numpy/matplotlib fails to install (building from source)

Problem: pip tries to compile numpy from source (Meson needs a compiler).

Fix: Use conda to install prebuilt packages:

conda create -n topicgen python=3.12 -y
conda activate topicgen
conda install -c conda-forge numpy matplotlib pandas -y
pip install crawl4ai requests beautifulsoup4 playwright
python -m playwright install chromium


Alternative pip approach: use a Python version with available wheels (3.12 recommended) and --prefer-binary:

python -m pip install --upgrade pip setuptools wheel
python -m pip install --prefer-binary numpy matplotlib pandas


If pip still tries to build, use conda.

2) llama-run.exe or local model errors

Ensure RUNNER and MODEL are correct and runnable from the shell where you run the script.

Try running RUNNER MODEL "hello" manually to validate the interface.

Increase timeout in run_model() if the local model is slow.

3) crawl4ai errors

Make sure Playwright is installed with python -m playwright install chromium.

If a site blocks the headless browser, try reducing concurrency or adding extra headers.

4) No links found from DuckDuckGo

Make sure query is valid; print query to verify.

If DuckDuckGo changes layout, adjust duckduckgo_search_links() selectors.

Performance tips

Reduce Phi calls by combining summary+topic into one prompt that returns JSON (faster, fewer model runs).

Reduce CONCURRENT_TASKS if you get blocked. Increase if you have fast machine and polite target sites.

Cache intermediate data/*.md if you plan multiple output experiments (the script purposely overwrites to keep things clean).

Example run (expected output)

Run: python all_in_one_pipeline.py

Enter your prompt: latest research on web crawling methods

Script prints progress:

⚡ Warm-up model...
Generating optimized query with Phi...
Saved query to query.txt
Searching DuckDuckGo for top links...
Saved 10 links to links.txt
Crawling links with crawl4ai (this may take a while)...
Saved 10 records to data/data.json
Phi asks: Choose output format: 1)... 
Your choice (1-5 or name): 5
Generating summaries and topic tags...
[1/10] summary_len=120 topic=web-crawling
...
Saved summaries to data/summaries.json
Saved table to data/table.csv (10 rows)
Saved pie chart to data/topic_pie.png
All done. Files are in the 'data' folder (and query.txt / links.txt).

Extending & Ideas (open to new ideas)

Combine summary+topic into one model call that returns JSON — reduces time by ~50%.

Add CLI args (argparse) to skip steps (e.g., only crawl, only produce outputs).

Save per-run timestamped snapshots (optional) instead of always overwriting.

Add robots.txt checking and exponential-backoff retry logic.

Add parallel model calls (careful — local runner may not support concurrency).

Add config file (config.yml) so non-dev users can tweak settings without editing the script.

Contributing

Open a PR or issue. Keep changes focused: smaller diffs, test locally before PR. If you add features that change file names or data formats, update this README accordingly.
